{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":791828,"sourceType":"datasetVersion","datasetId":119698}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom torchvision.models import resnet18, ResNet18_Weights\nimport torch\nimport PIL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:55:36.090946Z","iopub.execute_input":"2024-12-05T18:55:36.091271Z","iopub.status.idle":"2024-12-05T18:55:41.289572Z","shell.execute_reply.started":"2024-12-05T18:55:36.091241Z","shell.execute_reply":"2024-12-05T18:55:41.288842Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Попробуем построить модель для датасета пород собак. Сделаем Feature Extract через предобученный ResNet18 и обучим SVM.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\nDATASET_PATH = '/kaggle/input/stanford-dogs-dataset/images/Images'\n\ndata = {'img': [], 'breed': []}\n\nfor dirname, _, filenames in os.walk(DATASET_PATH):\n    for filename in filenames:\n        data['img'].append(os.path.join(dirname, filename))\n        data['breed'].append(os.path.basename(dirname).split('-', 1)[1])\n\ndf = pd.DataFrame.from_dict(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:56:08.065302Z","iopub.execute_input":"2024-12-05T18:56:08.065989Z","iopub.status.idle":"2024-12-05T18:57:21.687467Z","shell.execute_reply.started":"2024-12-05T18:56:08.065956Z","shell.execute_reply":"2024-12-05T18:57:21.686479Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.689318Z","iopub.execute_input":"2024-12-05T18:57:21.689697Z","iopub.status.idle":"2024-12-05T18:57:21.707566Z","shell.execute_reply.started":"2024-12-05T18:57:21.689660Z","shell.execute_reply":"2024-12-05T18:57:21.706758Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                     img       breed\n0      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound\n1      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound\n2      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound\n3      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound\n4      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound\n...                                                  ...         ...\n20575  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound\n20576  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound\n20577  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound\n20578  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound\n20579  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound\n\n[20580 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>breed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20575</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20576</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20577</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20578</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n    </tr>\n    <tr>\n      <th>20579</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n    </tr>\n  </tbody>\n</table>\n<p>20580 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df['breed'].unique()\nbreed_labels_map = {}\n\nunique_breeds = df['breed'].unique()\nfor i in range(len(unique_breeds)):\n    breed_labels_map[unique_breeds[i]] = i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.708642Z","iopub.execute_input":"2024-12-05T18:57:21.708895Z","iopub.status.idle":"2024-12-05T18:57:21.720825Z","shell.execute_reply.started":"2024-12-05T18:57:21.708871Z","shell.execute_reply":"2024-12-05T18:57:21.720133Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df['breed_label'] = df['breed'].apply(lambda x: breed_labels_map[x])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.722805Z","iopub.execute_input":"2024-12-05T18:57:21.723163Z","iopub.status.idle":"2024-12-05T18:57:21.741297Z","shell.execute_reply.started":"2024-12-05T18:57:21.723134Z","shell.execute_reply":"2024-12-05T18:57:21.740526Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                     img       breed  \\\n0      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound   \n1      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound   \n2      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound   \n3      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound   \n4      /kaggle/input/stanford-dogs-dataset/images/Ima...  otterhound   \n...                                                  ...         ...   \n20575  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound   \n20576  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound   \n20577  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound   \n20578  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound   \n20579  /kaggle/input/stanford-dogs-dataset/images/Ima...  bloodhound   \n\n       breed_label  \n0                0  \n1                0  \n2                0  \n3                0  \n4                0  \n...            ...  \n20575          119  \n20576          119  \n20577          119  \n20578          119  \n20579          119  \n\n[20580 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>breed</th>\n      <th>breed_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>otterhound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20575</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>20576</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>20577</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>20578</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>20579</th>\n      <td>/kaggle/input/stanford-dogs-dataset/images/Ima...</td>\n      <td>bloodhound</td>\n      <td>119</td>\n    </tr>\n  </tbody>\n</table>\n<p>20580 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass Dataset(Dataset):\n    def __init__(self, data, transform):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        image = PIL.Image.open(self.data.loc[idx, \"img\"]).convert('RGB')\n        image = self.transform(image)\n        label = torch.tensor(self.data.loc[idx, \"breed_label\"])\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.742318Z","iopub.execute_input":"2024-12-05T18:57:21.742620Z","iopub.status.idle":"2024-12-05T18:57:21.750167Z","shell.execute_reply.started":"2024-12-05T18:57:21.742596Z","shell.execute_reply":"2024-12-05T18:57:21.749349Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.751337Z","iopub.execute_input":"2024-12-05T18:57:21.751957Z","iopub.status.idle":"2024-12-05T18:57:21.766936Z","shell.execute_reply.started":"2024-12-05T18:57:21.751913Z","shell.execute_reply":"2024-12-05T18:57:21.766136Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torchvision.transforms import v2\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df[\"img\"], df[\"breed_label\"], random_state=42)\n\nX_train.index = np.arange(len(X_train))\ny_train.index = np.arange(len(y_train))\nX_test.index = np.arange(len(X_test))\ny_test.index = np.arange(len(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:21.767935Z","iopub.execute_input":"2024-12-05T18:57:21.768215Z","iopub.status.idle":"2024-12-05T18:57:22.300484Z","shell.execute_reply.started":"2024-12-05T18:57:21.768191Z","shell.execute_reply":"2024-12-05T18:57:22.299536Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"transform_train = v2.Compose([\n    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_test = v2.Compose([\n    v2.Resize(size=(224, 224)),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:22.301649Z","iopub.execute_input":"2024-12-05T18:57:22.302441Z","iopub.status.idle":"2024-12-05T18:57:22.354198Z","shell.execute_reply.started":"2024-12-05T18:57:22.302409Z","shell.execute_reply":"2024-12-05T18:57:22.353577Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_ds = Dataset(pd.concat([X_train, y_train], axis=1), transform_train)\ntest_ds = Dataset(pd.concat([X_test, y_test], axis=1), transform_test)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, num_workers=4)\ntest_loader = DataLoader(test_ds, batch_size=32, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:22.355166Z","iopub.execute_input":"2024-12-05T18:57:22.355503Z","iopub.status.idle":"2024-12-05T18:57:22.363305Z","shell.execute_reply.started":"2024-12-05T18:57:22.355465Z","shell.execute_reply":"2024-12-05T18:57:22.362411Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Extraction:\n    def __init__(self, network):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.network = network.eval().to(self.device)\n        \n    def extract(self, loader):\n        data_tmp = []\n        label_tmp = []\n\n        with torch.no_grad():\n            for x, y in loader:\n                x = x.to(self.device)\n            \n                outputs = self.network(x)\n                data_tmp.append(outputs.view(-1, 512).cpu().numpy())\n                \n                label_tmp.append(y.cpu().numpy())\n                \n        return np.vstack(data_tmp), np.hstack(label_tmp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:59:35.571128Z","iopub.execute_input":"2024-12-05T18:59:35.571481Z","iopub.status.idle":"2024-12-05T18:59:35.578337Z","shell.execute_reply.started":"2024-12-05T18:59:35.571452Z","shell.execute_reply":"2024-12-05T18:59:35.577508Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model = resnet18(weights=ResNet18_Weights.DEFAULT)\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:57:22.373219Z","iopub.execute_input":"2024-12-05T18:57:22.374057Z","iopub.status.idle":"2024-12-05T18:57:22.963824Z","shell.execute_reply.started":"2024-12-05T18:57:22.373995Z","shell.execute_reply":"2024-12-05T18:57:22.962950Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 187MB/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"ext = Extraction(feature_extractor)\n\ntrain_feature, train_label = ext.extract(train_loader)\ntest_feature, test_label = ext.extract(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:59:38.553548Z","iopub.execute_input":"2024-12-05T18:59:38.554359Z","iopub.status.idle":"2024-12-05T19:00:27.593431Z","shell.execute_reply.started":"2024-12-05T18:59:38.554321Z","shell.execute_reply":"2024-12-05T19:00:27.592267Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_feature.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:58:41.821325Z","iopub.execute_input":"2024-12-05T18:58:41.821679Z","iopub.status.idle":"2024-12-05T18:58:41.830746Z","shell.execute_reply.started":"2024-12-05T18:58:41.821653Z","shell.execute_reply":"2024-12-05T18:58:41.829842Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(15435, 512)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\nparam = {\n    \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n    #'C': [0.1, 0.5, 1, 2, 5, 10]\n}\n\nsvc = svm.SVC()\nsvm_grid = GridSearchCV(svc, param_grid=param, verbose=2, n_jobs=-1)\n\nsvm_grid.fit(train_feature, train_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:00:27.595505Z","iopub.execute_input":"2024-12-05T19:00:27.595816Z","iopub.status.idle":"2024-12-05T19:06:47.330200Z","shell.execute_reply.started":"2024-12-05T19:00:27.595787Z","shell.execute_reply":"2024-12-05T19:06:47.329185Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n             verbose=2)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={&#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;poly&#x27;, &#x27;rbf&#x27;, &#x27;sigmoid&#x27;]},\n             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={&#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;poly&#x27;, &#x27;rbf&#x27;, &#x27;sigmoid&#x27;]},\n             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}},{"name":"stdout","text":"[CV] END ......................................kernel=linear; total time=  50.7s\n[CV] END ......................................kernel=linear; total time=  51.1s\n[CV] END ........................................kernel=poly; total time=  55.2s\n[CV] END .........................................kernel=rbf; total time= 1.3min\n[CV] END .....................................kernel=sigmoid; total time= 1.3min\n[CV] END ......................................kernel=linear; total time=  52.4s\n[CV] END ........................................kernel=poly; total time=  56.8s\n[CV] END ........................................kernel=poly; total time=  53.6s\n[CV] END .........................................kernel=rbf; total time= 1.3min\n[CV] END .....................................kernel=sigmoid; total time= 1.3min\n[CV] END ......................................kernel=linear; total time=  52.7s\n[CV] END ........................................kernel=poly; total time=  56.5s\n[CV] END .........................................kernel=rbf; total time= 1.5min\n[CV] END .....................................kernel=sigmoid; total time= 1.3min\n[CV] END .....................................kernel=sigmoid; total time= 1.1min\n[CV] END ......................................kernel=linear; total time=  53.0s\n[CV] END ........................................kernel=poly; total time=  56.7s\n[CV] END .........................................kernel=rbf; total time= 1.5min\n[CV] END .........................................kernel=rbf; total time= 1.4min\n[CV] END .....................................kernel=sigmoid; total time= 1.0min\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"y_pred_svm_grid = svm_grid.predict(test_feature)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:21:52.889694Z","iopub.execute_input":"2024-12-05T19:21:52.890052Z","iopub.status.idle":"2024-12-05T19:22:43.658384Z","shell.execute_reply.started":"2024-12-05T19:21:52.889987Z","shell.execute_reply":"2024-12-05T19:22:43.657569Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n\naccuracy_score(test_label, y_pred_svm_grid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:22:47.512126Z","iopub.execute_input":"2024-12-05T19:22:47.513023Z","iopub.status.idle":"2024-12-05T19:22:47.519881Z","shell.execute_reply.started":"2024-12-05T19:22:47.512966Z","shell.execute_reply":"2024-12-05T19:22:47.518950Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.7904761904761904"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"labels = [i for i in range(120)]\n\ndef get_multilabel_accuracies(y_true, y_pred, labels):\n    cm = multilabel_confusion_matrix(test_label, y_pred, labels=labels)\n    total_count = len(y_true)\n    accuracies = []\n    precisions = []\n    recalls = []\n    for i in range(len(labels)):\n        true_positive_count = np.sum(cm[i,1,1]).item()\n        true_negative_count = np.sum(cm[i,0,0]).item()\n        false_positive_count = np.sum(cm[i,0,1]).item()\n        false_negative_count = np.sum(cm[i,1,0]).item()\n        \n        accuracy = (true_positive_count + true_negative_count) / total_count\n        precision = true_positive_count/(true_positive_count + false_positive_count)\n        recall = true_negative_count/(true_negative_count + false_positive_count)\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n    return accuracies, precisions, recalls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:26:52.831531Z","iopub.execute_input":"2024-12-05T19:26:52.831890Z","iopub.status.idle":"2024-12-05T19:26:52.838530Z","shell.execute_reply.started":"2024-12-05T19:26:52.831859Z","shell.execute_reply":"2024-12-05T19:26:52.837543Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"acs, precs, recs = get_multilabel_accuracies(test_label, y_pred_svm_grid, labels)\nprint(f'multiclass metrics\\naverage accuracy: {np.mean(acs)}\\naverage precision: {np.mean(precs)}\\naverage recall: {np.mean(recs)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:26:56.058657Z","iopub.execute_input":"2024-12-05T19:26:56.059256Z","iopub.status.idle":"2024-12-05T19:26:56.071746Z","shell.execute_reply.started":"2024-12-05T19:26:56.059221Z","shell.execute_reply":"2024-12-05T19:26:56.070772Z"}},"outputs":[{"name":"stdout","text":"multiclass metrics\naverage accuracy: 0.9965079365079366\naverage precision: 0.7939936658352342\naverage recall: 0.9982388498709386\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}