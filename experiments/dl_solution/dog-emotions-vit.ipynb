{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4969612,"sourceType":"datasetVersion","datasetId":2882322}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch.nn as nn\n\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights \nimport torch\nimport PIL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:45:42.400706Z","iopub.execute_input":"2025-05-11T18:45:42.401611Z","iopub.status.idle":"2025-05-11T18:45:42.406368Z","shell.execute_reply.started":"2025-05-11T18:45:42.401560Z","shell.execute_reply":"2025-05-11T18:45:42.405605Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\nDATASET_PATH = '/kaggle/input/dog-emotion/Dog Emotion/'\n\ndata = {'img': [], 'emotion': []}\n\nfor emotion in ['angry', 'happy', 'relaxed', 'sad']:\n    for dirname, _, filenames in os.walk(DATASET_PATH + emotion + '/'):\n        for filename in filenames:\n            data['img'].append(os.path.join(dirname, filename))\n            data['emotion'].append(emotion)\n\ndf = pd.DataFrame.from_dict(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:42:52.118655Z","iopub.execute_input":"2025-05-11T18:42:52.119412Z","iopub.status.idle":"2025-05-11T18:42:57.378400Z","shell.execute_reply.started":"2025-05-11T18:42:52.119380Z","shell.execute_reply":"2025-05-11T18:42:57.377700Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:42:57.380144Z","iopub.execute_input":"2025-05-11T18:42:57.380778Z","iopub.status.idle":"2025-05-11T18:42:57.397562Z","shell.execute_reply.started":"2025-05-11T18:42:57.380737Z","shell.execute_reply":"2025-05-11T18:42:57.396681Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                    img emotion\n0     /kaggle/input/dog-emotion/Dog Emotion/angry/RD...   angry\n1     /kaggle/input/dog-emotion/Dog Emotion/angry/GJ...   angry\n2     /kaggle/input/dog-emotion/Dog Emotion/angry/GX...   angry\n3     /kaggle/input/dog-emotion/Dog Emotion/angry/b3...   angry\n4     /kaggle/input/dog-emotion/Dog Emotion/angry/Hf...   angry\n...                                                 ...     ...\n3995  /kaggle/input/dog-emotion/Dog Emotion/sad/VNqe...     sad\n3996  /kaggle/input/dog-emotion/Dog Emotion/sad/0Bnt...     sad\n3997  /kaggle/input/dog-emotion/Dog Emotion/sad/aWoj...     sad\n3998  /kaggle/input/dog-emotion/Dog Emotion/sad/8ihS...     sad\n3999  /kaggle/input/dog-emotion/Dog Emotion/sad/TBHx...     sad\n\n[4000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/RD...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/GJ...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/GX...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/b3...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/Hf...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/VNqe...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/0Bnt...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/aWoj...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/8ihS...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/TBHx...</td>\n      <td>sad</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"emotion_labels_map = {}\n\nunique_emotions = df['emotion'].unique()\nfor i in range(len(unique_emotions)):\n    emotion_labels_map[unique_emotions[i]] = i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:42:57.398633Z","iopub.execute_input":"2025-05-11T18:42:57.398970Z","iopub.status.idle":"2025-05-11T18:42:57.413835Z","shell.execute_reply.started":"2025-05-11T18:42:57.398934Z","shell.execute_reply":"2025-05-11T18:42:57.413045Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df['emotion_label'] = df['emotion'].apply(lambda x: emotion_labels_map[x])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:42:58.239069Z","iopub.execute_input":"2025-05-11T18:42:58.239902Z","iopub.status.idle":"2025-05-11T18:42:58.252758Z","shell.execute_reply.started":"2025-05-11T18:42:58.239868Z","shell.execute_reply":"2025-05-11T18:42:58.251761Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                    img emotion  emotion_label\n0     /kaggle/input/dog-emotion/Dog Emotion/angry/RD...   angry              0\n1     /kaggle/input/dog-emotion/Dog Emotion/angry/GJ...   angry              0\n2     /kaggle/input/dog-emotion/Dog Emotion/angry/GX...   angry              0\n3     /kaggle/input/dog-emotion/Dog Emotion/angry/b3...   angry              0\n4     /kaggle/input/dog-emotion/Dog Emotion/angry/Hf...   angry              0\n...                                                 ...     ...            ...\n3995  /kaggle/input/dog-emotion/Dog Emotion/sad/VNqe...     sad              3\n3996  /kaggle/input/dog-emotion/Dog Emotion/sad/0Bnt...     sad              3\n3997  /kaggle/input/dog-emotion/Dog Emotion/sad/aWoj...     sad              3\n3998  /kaggle/input/dog-emotion/Dog Emotion/sad/8ihS...     sad              3\n3999  /kaggle/input/dog-emotion/Dog Emotion/sad/TBHx...     sad              3\n\n[4000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>emotion</th>\n      <th>emotion_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/RD...</td>\n      <td>angry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/GJ...</td>\n      <td>angry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/GX...</td>\n      <td>angry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/b3...</td>\n      <td>angry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/angry/Hf...</td>\n      <td>angry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/VNqe...</td>\n      <td>sad</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/0Bnt...</td>\n      <td>sad</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/aWoj...</td>\n      <td>sad</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/8ihS...</td>\n      <td>sad</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>/kaggle/input/dog-emotion/Dog Emotion/sad/TBHx...</td>\n      <td>sad</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass Dataset(Dataset):\n    def __init__(self, data, transform):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        image = PIL.Image.open(self.data.loc[idx, \"img\"]).convert('RGB')\n        image = self.transform(image)\n        label = torch.tensor(self.data.loc[idx, \"emotion_label\"])\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:42:59.878311Z","iopub.execute_input":"2025-05-11T18:42:59.878759Z","iopub.status.idle":"2025-05-11T18:42:59.884287Z","shell.execute_reply.started":"2025-05-11T18:42:59.878724Z","shell.execute_reply":"2025-05-11T18:42:59.883496Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torchvision.transforms import v2\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df[\"img\"], df[\"emotion_label\"], random_state=42)\n\nX_train.index = np.arange(len(X_train))\ny_train.index = np.arange(len(y_train))\nX_test.index = np.arange(len(X_test))\ny_test.index = np.arange(len(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:43:02.512761Z","iopub.execute_input":"2025-05-11T18:43:02.513194Z","iopub.status.idle":"2025-05-11T18:43:03.236050Z","shell.execute_reply.started":"2025-05-11T18:43:02.513153Z","shell.execute_reply":"2025-05-11T18:43:03.235089Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"transform_train = transform_test = v2.Compose([\n    v2.Resize(size=(224, 224)),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.RandomResizedCrop(224),  # Random crop with resize to 224x224\n    v2.RandomHorizontalFlip(p=0.5),  # Horizontal flip with 50% probability\n    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n    v2.RandomRotation(7),\n    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_test = v2.Compose([\n    v2.Resize(size=(224, 224)),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:40:20.779834Z","iopub.execute_input":"2025-05-11T19:40:20.780778Z","iopub.status.idle":"2025-05-11T19:40:20.788826Z","shell.execute_reply.started":"2025-05-11T19:40:20.780741Z","shell.execute_reply":"2025-05-11T19:40:20.787988Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"train_ds = Dataset(pd.concat([X_train, y_train], axis=1), transform_train)\ntest_ds = Dataset(pd.concat([X_test, y_test], axis=1), transform_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:40:38.547727Z","iopub.execute_input":"2025-05-11T19:40:38.548621Z","iopub.status.idle":"2025-05-11T19:40:38.556027Z","shell.execute_reply.started":"2025-05-11T19:40:38.548576Z","shell.execute_reply":"2025-05-11T19:40:38.555209Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nmodel = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\nmodel.heads.head = nn.Linear(in_features=768, out_features=4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nnum_epochs = 10\nbatch_size = 32\nlearning_rate = 1e-5\nweight_decay = 0.0\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, drop_last=True)\ntest_loader = DataLoader(test_ds, batch_size=batch_size)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for i, (images, labels) in enumerate(train_loader):\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        if (i+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n                  f'Loss: {running_loss/100:.4f}, '\n                  f'Accuracy: {100 * correct/total:.2f}%')\n            running_loss = 0.0\n    \nprint('Training complete')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:40:43.647414Z","iopub.execute_input":"2025-05-11T19:40:43.648277Z","iopub.status.idle":"2025-05-11T19:55:14.129774Z","shell.execute_reply.started":"2025-05-11T19:40:43.648243Z","shell.execute_reply":"2025-05-11T19:55:14.128538Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [10/93], Loss: 0.1361, Accuracy: 30.62%\nEpoch [1/10], Step [20/93], Loss: 0.1354, Accuracy: 30.78%\nEpoch [1/10], Step [30/93], Loss: 0.1255, Accuracy: 35.52%\nEpoch [1/10], Step [40/93], Loss: 0.1210, Accuracy: 38.52%\nEpoch [1/10], Step [50/93], Loss: 0.1105, Accuracy: 42.12%\nEpoch [1/10], Step [60/93], Loss: 0.1071, Accuracy: 45.00%\nEpoch [1/10], Step [70/93], Loss: 0.1028, Accuracy: 47.14%\nEpoch [1/10], Step [80/93], Loss: 0.0961, Accuracy: 49.02%\nEpoch [1/10], Step [90/93], Loss: 0.0888, Accuracy: 51.32%\nEpoch [2/10], Step [10/93], Loss: 0.0790, Accuracy: 69.38%\nEpoch [2/10], Step [20/93], Loss: 0.0765, Accuracy: 69.38%\nEpoch [2/10], Step [30/93], Loss: 0.0757, Accuracy: 69.90%\nEpoch [2/10], Step [40/93], Loss: 0.0717, Accuracy: 70.78%\nEpoch [2/10], Step [50/93], Loss: 0.0708, Accuracy: 71.19%\nEpoch [2/10], Step [60/93], Loss: 0.0676, Accuracy: 71.82%\nEpoch [2/10], Step [70/93], Loss: 0.0696, Accuracy: 72.05%\nEpoch [2/10], Step [80/93], Loss: 0.0635, Accuracy: 72.62%\nEpoch [2/10], Step [90/93], Loss: 0.0560, Accuracy: 73.26%\nEpoch [3/10], Step [10/93], Loss: 0.0599, Accuracy: 78.75%\nEpoch [3/10], Step [20/93], Loss: 0.0609, Accuracy: 77.03%\nEpoch [3/10], Step [30/93], Loss: 0.0643, Accuracy: 76.67%\nEpoch [3/10], Step [40/93], Loss: 0.0579, Accuracy: 77.19%\nEpoch [3/10], Step [50/93], Loss: 0.0544, Accuracy: 77.00%\nEpoch [3/10], Step [60/93], Loss: 0.0539, Accuracy: 77.50%\nEpoch [3/10], Step [70/93], Loss: 0.0588, Accuracy: 77.05%\nEpoch [3/10], Step [80/93], Loss: 0.0574, Accuracy: 76.95%\nEpoch [3/10], Step [90/93], Loss: 0.0469, Accuracy: 77.47%\nEpoch [4/10], Step [10/93], Loss: 0.0485, Accuracy: 82.19%\nEpoch [4/10], Step [20/93], Loss: 0.0586, Accuracy: 80.78%\nEpoch [4/10], Step [30/93], Loss: 0.0491, Accuracy: 80.83%\nEpoch [4/10], Step [40/93], Loss: 0.0491, Accuracy: 80.86%\nEpoch [4/10], Step [50/93], Loss: 0.0456, Accuracy: 81.44%\nEpoch [4/10], Step [60/93], Loss: 0.0493, Accuracy: 81.88%\nEpoch [4/10], Step [70/93], Loss: 0.0447, Accuracy: 81.96%\nEpoch [4/10], Step [80/93], Loss: 0.0484, Accuracy: 81.99%\nEpoch [4/10], Step [90/93], Loss: 0.0409, Accuracy: 82.47%\nEpoch [5/10], Step [10/93], Loss: 0.0460, Accuracy: 83.44%\nEpoch [5/10], Step [20/93], Loss: 0.0478, Accuracy: 82.19%\nEpoch [5/10], Step [30/93], Loss: 0.0502, Accuracy: 81.35%\nEpoch [5/10], Step [40/93], Loss: 0.0454, Accuracy: 81.88%\nEpoch [5/10], Step [50/93], Loss: 0.0419, Accuracy: 82.12%\nEpoch [5/10], Step [60/93], Loss: 0.0455, Accuracy: 82.50%\nEpoch [5/10], Step [70/93], Loss: 0.0496, Accuracy: 82.23%\nEpoch [5/10], Step [80/93], Loss: 0.0443, Accuracy: 82.42%\nEpoch [5/10], Step [90/93], Loss: 0.0453, Accuracy: 82.43%\nEpoch [6/10], Step [10/93], Loss: 0.0364, Accuracy: 86.88%\nEpoch [6/10], Step [20/93], Loss: 0.0452, Accuracy: 85.47%\nEpoch [6/10], Step [30/93], Loss: 0.0410, Accuracy: 85.10%\nEpoch [6/10], Step [40/93], Loss: 0.0438, Accuracy: 84.69%\nEpoch [6/10], Step [50/93], Loss: 0.0423, Accuracy: 84.38%\nEpoch [6/10], Step [60/93], Loss: 0.0408, Accuracy: 83.96%\nEpoch [6/10], Step [70/93], Loss: 0.0463, Accuracy: 83.62%\nEpoch [6/10], Step [80/93], Loss: 0.0376, Accuracy: 83.91%\nEpoch [6/10], Step [90/93], Loss: 0.0386, Accuracy: 83.96%\nEpoch [7/10], Step [10/93], Loss: 0.0363, Accuracy: 86.56%\nEpoch [7/10], Step [20/93], Loss: 0.0362, Accuracy: 86.88%\nEpoch [7/10], Step [30/93], Loss: 0.0369, Accuracy: 86.67%\nEpoch [7/10], Step [40/93], Loss: 0.0351, Accuracy: 86.64%\nEpoch [7/10], Step [50/93], Loss: 0.0390, Accuracy: 86.44%\nEpoch [7/10], Step [60/93], Loss: 0.0407, Accuracy: 85.99%\nEpoch [7/10], Step [70/93], Loss: 0.0384, Accuracy: 85.98%\nEpoch [7/10], Step [80/93], Loss: 0.0379, Accuracy: 85.90%\nEpoch [7/10], Step [90/93], Loss: 0.0286, Accuracy: 86.11%\nEpoch [8/10], Step [10/93], Loss: 0.0287, Accuracy: 89.38%\nEpoch [8/10], Step [20/93], Loss: 0.0298, Accuracy: 89.22%\nEpoch [8/10], Step [30/93], Loss: 0.0361, Accuracy: 88.54%\nEpoch [8/10], Step [40/93], Loss: 0.0385, Accuracy: 87.73%\nEpoch [8/10], Step [50/93], Loss: 0.0318, Accuracy: 88.00%\nEpoch [8/10], Step [60/93], Loss: 0.0349, Accuracy: 87.81%\nEpoch [8/10], Step [70/93], Loss: 0.0351, Accuracy: 87.59%\nEpoch [8/10], Step [80/93], Loss: 0.0367, Accuracy: 87.30%\nEpoch [8/10], Step [90/93], Loss: 0.0248, Accuracy: 87.81%\nEpoch [9/10], Step [10/93], Loss: 0.0255, Accuracy: 90.62%\nEpoch [9/10], Step [20/93], Loss: 0.0287, Accuracy: 89.69%\nEpoch [9/10], Step [30/93], Loss: 0.0315, Accuracy: 89.90%\nEpoch [9/10], Step [40/93], Loss: 0.0269, Accuracy: 90.39%\nEpoch [9/10], Step [50/93], Loss: 0.0276, Accuracy: 90.38%\nEpoch [9/10], Step [60/93], Loss: 0.0350, Accuracy: 89.58%\nEpoch [9/10], Step [70/93], Loss: 0.0342, Accuracy: 89.15%\nEpoch [9/10], Step [80/93], Loss: 0.0312, Accuracy: 89.02%\nEpoch [9/10], Step [90/93], Loss: 0.0294, Accuracy: 89.13%\nEpoch [10/10], Step [10/93], Loss: 0.0261, Accuracy: 90.31%\nEpoch [10/10], Step [20/93], Loss: 0.0332, Accuracy: 89.06%\nEpoch [10/10], Step [30/93], Loss: 0.0281, Accuracy: 89.17%\nEpoch [10/10], Step [40/93], Loss: 0.0336, Accuracy: 88.44%\nEpoch [10/10], Step [50/93], Loss: 0.0295, Accuracy: 88.62%\nEpoch [10/10], Step [60/93], Loss: 0.0337, Accuracy: 88.54%\nEpoch [10/10], Step [70/93], Loss: 0.0285, Accuracy: 88.57%\nEpoch [10/10], Step [80/93], Loss: 0.0321, Accuracy: 88.52%\nEpoch [10/10], Step [90/93], Loss: 0.0309, Accuracy: 88.75%\nTraining complete\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model.eval()\n\npredictions = []\ntrue_labels = []\n\nfor images, labels in test_loader:\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    predictions += predicted.cpu().tolist()\n    true_labels += labels.cpu().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:55:14.131799Z","iopub.execute_input":"2025-05-11T19:55:14.132217Z","iopub.status.idle":"2025-05-11T19:55:27.060074Z","shell.execute_reply.started":"2025-05-11T19:55:14.132167Z","shell.execute_reply":"2025-05-11T19:55:27.059062Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:55:27.061479Z","iopub.execute_input":"2025-05-11T19:55:27.061812Z","iopub.status.idle":"2025-05-11T19:55:27.065931Z","shell.execute_reply.started":"2025-05-11T19:55:27.061773Z","shell.execute_reply":"2025-05-11T19:55:27.065100Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"accuracy_score(predictions, true_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:55:27.067741Z","iopub.execute_input":"2025-05-11T19:55:27.068023Z","iopub.status.idle":"2025-05-11T19:55:27.080608Z","shell.execute_reply.started":"2025-05-11T19:55:27.067982Z","shell.execute_reply":"2025-05-11T19:55:27.079703Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.858"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'animals_vit_weights.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:16:51.715580Z","iopub.execute_input":"2025-05-11T19:16:51.716272Z","iopub.status.idle":"2025-05-11T19:16:52.258267Z","shell.execute_reply.started":"2025-05-11T19:16:51.716238Z","shell.execute_reply":"2025-05-11T19:16:52.257557Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'animals_vit_weights.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:23:11.441452Z","iopub.execute_input":"2025-05-11T19:23:11.442173Z","iopub.status.idle":"2025-05-11T19:23:11.448322Z","shell.execute_reply.started":"2025-05-11T19:23:11.442136Z","shell.execute_reply":"2025-05-11T19:23:11.447466Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/animals_vit_weights.pt","text/html":"<a href='animals_vit_weights.pt' target='_blank'>animals_vit_weights.pt</a><br>"},"metadata":{}}],"execution_count":34}]}